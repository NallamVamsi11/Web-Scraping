{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d667135",
   "metadata": {},
   "source": [
    "# Web Scraping from GitHub Topics.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff28ee9",
   "metadata": {},
   "source": [
    "#### WebScraping - Web scraping (or data scraping) is a technique used to collect content and data from the internet. This data is usually saved in a local file so that it can be manipulated and analyzed as needed.\n",
    "\n",
    "#### GitHub - GitHub is a code hosting platform for collaboration and version control.A GitHub repository can be used to store a development project.It can contain folders and any type of files (HTML, CSS, JavaScript, Documents, Data, Images).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808acf55",
   "metadata": {},
   "source": [
    "### Objective:- We will scrape data from GitHub Topics. We will collect the list of Topics and their Descriptions and their Url's and for each Topic we will scrape the Top repositories and their user name and their url and no.of  stars they have.\n",
    "                 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00a8421",
   "metadata": {},
   "source": [
    "#### We use Python and beautifulsoup , requests , os , pandas , csv modules in python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92afb8f",
   "metadata": {},
   "source": [
    "### - Project Outline\n",
    "- From GITHUB we will scrape each topic and its top repositories details- https://github.com/topics\n",
    "- topic name , topic url , topic discription and its top repositories.\n",
    "- for each repository we will have repo name , username , no.of stars , repo url.\n",
    "- for each topic we will create a csv file. ''' Repo_Name,User_Name,No_OF_Stars,Repo_URL '''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a33dff5",
   "metadata": {},
   "source": [
    "#### we use libraries:-\n",
    "#### requests - to download the web page.\n",
    "#### beautifulsoup - to parse through the data and extract information.\n",
    "#### pandas - to create a pandas Dataframe.\n",
    "#### A csv file to store the data in tabular format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50ec53b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#since we already have requests library, or else - !pip install requests.\n",
    "#the requests.get() will return a response object - 'resp_obj'  -<Response [200]>\n",
    "#using resp_obj.text we can print all data inside the response object that it collected from url.\n",
    "#len(resp_obj.text) - 142575 characters\n",
    "#but not recommended\n",
    "#resp_obj.status_code - 200\n",
    "'''http status codes\n",
    "Informational responses (100–199)\n",
    "Successful responses (200–299)\n",
    "Redirects (300–399)\n",
    "Client errors (400–499)\n",
    "Server errors (500–599)'''\n",
    "#to install beautifulsoup\n",
    "#!pip install beautifulsoup4 --quiet\n",
    "#frame=pd.DataFrame(dict) creating a pandas dataframe\n",
    "#frame.to_csv('name.csv',index=None) index=None removes the indexes in csv file\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a84cafb",
   "metadata": {},
   "source": [
    "## Final Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6dfcb5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import requests\n",
    "import pandas as pd\n",
    "import os\n",
    "#function to get titles of all topics in topic page.\n",
    "def get_topics_titles(doc):\n",
    "    topic_title_tags=doc.find_all('p',{'class':\"f3 lh-condensed mb-0 mt-1 Link--primary\"})\n",
    "    topic_titles=[]\n",
    "    for i in topic_title_tags:\n",
    "        topic_titles.append(i.text)\n",
    "    return topic_titles\n",
    "\n",
    "#function to get description of all topics.\n",
    "def get_topics_descriptions(doc):\n",
    "    topic_desc_tags=doc.find_all('p',{'class':'f5 color-text-secondary mb-0 mt-1'})\n",
    "    topic_descriptions=[]\n",
    "    for i in topic_desc_tags:\n",
    "        topic_descriptions.append(i.text.strip())\n",
    "    return topic_descriptions\n",
    "\n",
    "#function to get topics urls.\n",
    "def get_topics_urls(doc):\n",
    "    topic_link_tags=doc.find_all('a',{'class':'d-flex no-underline'})\n",
    "    topic_links=[]\n",
    "    for i in topic_link_tags:\n",
    "        url='https://github.com'+i['href']\n",
    "        topic_links.append(url)\n",
    "    return topic_links\n",
    "\n",
    "#function to give a repo details\n",
    "def repo_details(h3_tag,s_tag):\n",
    "    a_tags=h3_tag.find_all('a')\n",
    "    user_name=a_tags[0].text.strip()\n",
    "    repo_name=a_tags[1].text.strip()\n",
    "    repo_url='https://github.com'+a_tags[1]['href']\n",
    "    if s_tag.text.strip()[-1]=='k':\n",
    "        stars=int(float(s_tag.text.strip()[:-1])*1000)\n",
    "    else:\n",
    "        stars=(int(s_tag.text.strip()))\n",
    "    return repo_name,user_name,repo_url,stars\n",
    "\n",
    "#function to get repository details of each topic.\n",
    "def get_topic_repos(topic_url,name):\n",
    "    response=requests.get(topic_url)\n",
    "    if response.status_code!=200:\n",
    "        print('-->>sorry we are unable to load','\"',name,'\"','topic.')\n",
    "        pass\n",
    "    else:\n",
    "        topic_doc=BeautifulSoup(response.text,'html.parser')\n",
    "        repo_tags=topic_doc.find_all('h3',{'class':'f3 color-text-secondary text-normal lh-condensed'})\n",
    "        star_tags=topic_doc.find_all('a',{'class':'social-count float-none'})\n",
    "        repository_dict={'repo_name':[],'user_name':[],'repo_url':[],'no_stars':[]}\n",
    "        for i in range(len(repo_tags)):\n",
    "            reps=repo_details(repo_tags[i],star_tags[i])\n",
    "            repository_dict['repo_name'].append(reps[0])\n",
    "            repository_dict['user_name'].append(reps[1])\n",
    "            repository_dict['repo_url'].append(reps[2])\n",
    "            repository_dict['no_stars'].append(reps[3])\n",
    "        repository_details=pd.DataFrame(repository_dict)\n",
    "        fname=name+'repository.csv'\n",
    "        #if os.path.exists(fname):\n",
    "            #print('This file already exists.Skipping...')\n",
    "            #return repository_details\n",
    "        #if the if block executed i will not update the data inside the files so its good to update file everytime.\n",
    "        repository_details.to_csv(fname,index=None)\n",
    "        print('----------------Scrapping Successful of {}'.format(name))\n",
    "        return repository_details\n",
    "\n",
    "#function to scrape topics,\n",
    "def get_topics_details(topics_url):\n",
    "    resp_obj=requests.get(topics_url)\n",
    "    page_contents=resp_obj.text\n",
    "    doc = BeautifulSoup(page_contents, 'html.parser')\n",
    "    topics_dict={'topic_titles':get_topics_titles(doc),\n",
    "                 'topic_descriptions':get_topics_descriptions(doc),\n",
    "                 'topic_links':get_topics_urls(doc)}\n",
    "    topics_det=pd.DataFrame(topics_dict)\n",
    "    topics_det.to_csv('topics_github.csv',index=None)\n",
    "    return topics_dict['topic_links'],topics_dict['topic_titles']\n",
    "#main function\n",
    "def web_scrapping_result():\n",
    "    url='https://github.com/topics'\n",
    "    topicslinks,topicsnames=get_topics_details(url)\n",
    "    final_data=[]\n",
    "    for i in range(len(topicslinks)):\n",
    "        print('Scrapping top repos of {}'.format(topicsnames[i]))\n",
    "        p=get_topic_repos(topicslinks[i],topicsnames[i])\n",
    "        final_data.append(p)\n",
    "    return final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "273b45f4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9100/175935132.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlist_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mweb_scrapping_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9100/3391810392.py\u001b[0m in \u001b[0;36mweb_scrapping_result\u001b[1;34m()\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mweb_scrapping_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m     \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'https://github.com/topics'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m     \u001b[0mtopicslinks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtopicsnames\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mget_topics_details\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m     \u001b[0mfinal_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtopicslinks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9100/3391810392.py\u001b[0m in \u001b[0;36mget_topics_details\u001b[1;34m(topics_url)\u001b[0m\n\u001b[0;32m     76\u001b[0m                  \u001b[1;34m'topic_descriptions'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mget_topics_descriptions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m                  'topic_links':get_topics_urls(doc)}\n\u001b[1;32m---> 78\u001b[1;33m     \u001b[0mtopics_det\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtopics_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m     \u001b[0mtopics_det\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'topics_github.csv'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtopics_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'topic_links'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtopics_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'topic_titles'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    612\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    613\u001b[0m             \u001b[1;31m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 614\u001b[1;33m             \u001b[0mmgr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict_to_mgr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtyp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmanager\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    615\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    616\u001b[0m             \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmrecords\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36mdict_to_mgr\u001b[1;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[0;32m    462\u001b[0m         \u001b[1;31m# TODO: can we get rid of the dt64tz special case above?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 464\u001b[1;33m     return arrays_to_mgr(\n\u001b[0m\u001b[0;32m    465\u001b[0m         \u001b[0marrays\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtyp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtyp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconsolidate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    466\u001b[0m     )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, arr_names, index, columns, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[0;32m    117\u001b[0m         \u001b[1;31m# figure out the index, if necessary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m             \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_extract_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m             \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36m_extract_index\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    633\u001b[0m             \u001b[0mlengths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_lengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    634\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 635\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"All arrays must be of the same length\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    636\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    637\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhave_dicts\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: All arrays must be of the same length"
     ]
    }
   ],
   "source": [
    "list_data=web_scrapping_result()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72bbd0e3",
   "metadata": {},
   "source": [
    "#### Importing required libraries.\n",
    "- from bs4 import BeautifulSoup\n",
    "- import csv\n",
    "- import requests\n",
    "- import pandas as pd\n",
    "- import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac90d5a",
   "metadata": {},
   "source": [
    "#### our code starts from calling - web_scrapping_result() function\n",
    "####  url='https://github.com/topics' - this is the url of topics from github we will scrape the data from this page.\n",
    "#### get_topics_details(url) this will call get_topic_details function with url as argument\n",
    "- ---resp_obj=requests.get(topics_url) , requests.get() will read the webpage and returns a response object.\n",
    "- ---page_contents=resp_obj.text , we are storing all text data in the object in page_contents.type(page_contents)-str.\n",
    "- ---doc = BeautifulSoup(page_contents, 'html.parser') ,Beautiful Soup is a Python library for pulling data out of HTML and -     XML files.It will parse the data in page_contents in html format, alligns the data in a perfect order, so that we can -     extract data easily and return in a beautifulsoup object. For more info look into - https://beautiful-soup-4.readthedocs.io/en/latest/ .\n",
    "- ---topics_dict , we are creating a dictionary of lists returned by functions:-\n",
    "- ---get_topics_titles(doc),get_topics_descriptions(doc),get_topics_urls(doc) , for creating a pandas dataframe.\n",
    "- ---get_topics_titles(doc):-\n",
    "- ---------This function will take the beautifulsoup object as input.\n",
    "- ---------topic_title_tags=doc.find_all('p',{'class':\"f3 lh-condensed mb-0 mt-1 Link--primary\"}) , the find_all() will                return all 'p' tags with class attribute- \"f3 lh-condensed mb-0 mt-1 Link--primary\"} , because those p tags   -            will contain the name of Topic in them.\n",
    "- ---------topic_titles=[] ,we are creating an empty list.\n",
    "           for i in topic_title_tags:\n",
    "               topic_titles.append(i.text), i.text will give the name of topic inside a p tag line.because in a p tag name                only is stored in string(text) format.\n",
    "- ---------in a similar way we implement get_topics_descriptions(doc),get_topics_urls(doc) these functions with input as beautifulsoup object and all 3 will return lists of names of topics , descriptions of topics , urls of topics.\n",
    "- ---topics_det=pd.DataFrame(topics_dict) , pd.DataFrame() will return a pandas data frame by taking the dict of lists we created.\n",
    "- ---topics_det.to_csv('topics_github.csv',index=None) , we write the dataframe to a csv file , if file not exists noproblem this statment will create the file.\n",
    "- ---return topics_dict['topic_links'],topics_dict['topic_titles'] \n",
    "- ---finally we return the list of topics links and topics titles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf3ed48",
   "metadata": {},
   "source": [
    "### from calling the get_topics_details() , we successfully created a csv file with topics details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4c5d78",
   "metadata": {},
   "source": [
    "#### let us understand the remaining part of web_scrapping_result() function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b3d428",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "    for i in range(len(topicslinks)):\n",
    "    \n",
    "        print('Scrapping top repos of {}'.format(topicsnames[i]))\n",
    "        \n",
    "        p=get_topic_repos(topicslinks[i],topicsnames[i])\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e609f2",
   "metadata": {},
   "source": [
    "- now ,  topicslinks contain urls of all topics and topicsnames contains names of all topics\n",
    "- now using for loop we are parsing and calling get_topic_repos(url,name) function \n",
    "- get_topic_repos(topicslinks[i] , topicsnames[i])  takes 2 arguments , url of topic and name of topic.\n",
    "- def get_topic_repos(topic_url,name):\n",
    "-    response=requests.get(topic_url) , this will return a response object for the data in topic url.\n",
    "### each topic page will contain  repositories , we have to extract the data we need (about top repositories) from the webpage data.\n",
    "- response.status_code!=200: , if status code is not 200 , we are unable to load the page.\n",
    "- if status code 200 , we successfully reed the data from webpage.\n",
    "- topic_doc=BeautifulSoup(response.text,'html.parser') ,   we are creating beautifulsoup object from html text.\n",
    "- repo_tags=topic_doc.find_all('h3',{'class':'f3 color-text-secondary text-normal lh-condensed'}) ,   we get a list of all 'h3' tags with class attribute-'f3 color-text-secondary text-normal lh-condensed' , because those tags contains the data for repository name , username , url and other data.\n",
    "- star_tags=topic_doc.find_all('a',{'class':'social-count float-none'}) ,   we get a list of all 'a' tags with class attriute ,'social-count float-none' , because those tags contains about no.of stars each repository have.\n",
    "- repository_dict={'repo_name':[],'user_name':[],'repo_url':[],'no_stars':[]} , creating a dict for dataframe.\n",
    "- for i in range(len(repo_tags)):\n",
    "            reps=repo_details(repo_tags[i],star_tags[i]) , we call repo_details() function with inputs as a repo_tag and star_tag.Because we know , repo_tag contain info about repository name , user name , url etc.. , star_tag contain no.of stars.\n",
    "- --------def repo_details(h3_tag,s_tag): , function with 2 args.\n",
    "- --------a_tags=h3_tag.find_all('a') , each h3 tag is a parent tag of 2 'a' tags. \n",
    "- --------The 1st a-tag contains info about username in text format.\n",
    "- --------The 2nd a-tag contains info about repository name in text format, and its url in a 'href' attribute.\n",
    "- --------user_name=a_tags[0].text.strip()\n",
    "- --------repo_name=a_tags[1].text.strip()\n",
    "- --------repo_url='https://github.com'+ a_tags[1]['href']\n",
    "- --------if s_tag.text.strip()[-1]=='k':\n",
    "- --------stars=int(float(s_tag.text.strip()[:-1])*1000) , since the star count will be in text format we are converting it into integer.\n",
    "- --------return repo_name,user_name,repo_url,stars \n",
    "- --------finally repo_details function will return repository name , username, url , no.of stars.\n",
    "- reps=repo_details(repo_tags[i],star_tags[i]) , we store the returned value in a tuple-reps.\n",
    "-            repository_dict['repo_name'].append(reps[0])\n",
    "-            repository_dict['user_name'].append(reps[1])\n",
    "-            repository_dict['repo_url'].append(reps[2])\n",
    "-            repository_dict['no_stars'].append(reps[3])\n",
    "- one by one , we append the returned details into the lists of dict we created for dataframe conversion.\n",
    "- repository_details=pd.DataFrame(repository_dict) ,using the dict we create a data frame object.\n",
    "-        fname=name+'repository.csv' , this is just the names we want for our csv file.\n",
    "- repository_details.to_csv(fname,index=None) , creating a csv file with details of top repositories for a topic.\n",
    "- return repository_details , we return the dataframe object.\n",
    "- p=get_topic_repos(topicslinks[i],topicsnames[i]) , we store the dataframe object in p variable \n",
    "-        final_data.append(p) , and append it to the list we created.\n",
    "- return final_data ,finally web_scrapping_result() will return a list of dataframe's that contains each topics top repository details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e1e5d3",
   "metadata": {},
   "source": [
    "- we can also remove the final_data list from code , its just for gathering the dataframes, no need to keep it we have the same data in csv files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c989ab44",
   "metadata": {},
   "source": [
    "### Code End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0626ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
